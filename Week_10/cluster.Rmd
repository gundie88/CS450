---
title: "cluster"
output:
  word_document:
    toc: yes
  html_document:
    code_folding: hide
    theme: cerulean
    toc: yes
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

```{r}
library(datasets)
library(cluster)
library(dplyr)
library(tidyverse)
library(RColorBrewer)
library(animation)
```

# AGGLOMERATIVE HIERARCHICAL CLUSTERING


Step 1<br />
```{r}
##Step 1 
data = state.x77
data <- data.frame(data)
```

Step 2<br />
```{r}
##Step 2
#Dendrogram of non-normalized data
# first compute a distance matrix
distance = dist(as.matrix(data))
# now perform the clustering
hc = hclust(distance)
# finally, plot the dendrogram
plot(hc)
```

Step 3<br />
```{r}
##Step 3
#Dendrogram of normalized data
#summary(data)
#Scaling the data
data_scaled <- data %>%
  mutate(pop_scal = scale(Population),
         income_scal = scale(Income),
         illit_scal = scale(Illiteracy),
         life_scal = scale(Life.Exp),
         murder_scal = scale(Murder),
         grad_scal = scale(HS.Grad),
         frost_scal = scale(Frost),
         area_scal = scale(Area)) %>%
  select(-c(Population, Income, Illiteracy, Life.Exp, 
            Murder, HS.Grad, Frost, Area))

# distance matrix
distance_n = dist(as.matrix(data_scaled))
# perform clustering
hc_n = hclust(distance_n)
# plot the dendrogram
plot(hc_n)

```

When I scaled the data it took away the name of the states in alphabetical order and assigned them a number that corresponds with its place alphabetically.When the data was normalized the clustering changed. The normalized data seemed to do a better job of clustering into more distinct groups. It seems that the attributes impact the clustering more when its normalized. 

Step 4<br />
```{r}
##Step 4
data_scaled_a <- data_scaled %>% 
  select(-area_scal)
# compute a distance matrix
distance = dist(as.matrix(data_scaled_a))
# perform the clustering
hc_a = hclust(distance)
# plot the dendrogram
plot(hc_a)
```
When area was removed, the clusters seemed to be more balanced across the dendrogram. It makes sense that when you remove the area of each state it everything seems more balanced, this is because not each state has the same area.

Step 5<br />
```{r}
##Step 5
data_scaled_f <- data_scaled[,7]
# compute a distance matrix
distance = dist(as.matrix(data_scaled_f))
# perform the clustering
hc_f = hclust(distance)
# plot the dendrogram
plot(hc_f)
```
States with similar weather patterns are clustered together.

# USING K-MEANS


Step 1<br />
```{r}
##Step 1.
#use nomralized data 
```

Step 2<br />
```{r}
##Step 2.
# Cluster into k=3 clusters:
pc_cluster = kmeans(data_scaled, 3)
summary(pc_cluster)
print(pc_cluster$cluster)
print(pc_cluster$centers)
print(pc_cluster$size)
clusplot(data_scaled, pc_cluster$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```
I would think they are grouped this way because population plays a big factor in their groupings.

Step 3<br />
```{r}
#Step 3.
# Cluster into k=3 clusters:
pc_cluster = kmeans(data_scaled, 3)
# compute the total within clusters sum of squares
kmean_withinss <- function(k) {
  cluster <- kmeans(data_scaled, k)
  return (cluster$tot.withinss)
}
# Set maximum cluster 
max_k <-25 
# Run algorithm over a range of k 
wss <- sapply(1:max_k, kmean_withinss)
elbow <-data.frame(1:max_k, wss)
ggplot(elbow, aes(x = X1.max_k, y = wss)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq(1, 26, by = 1)) +
  theme_bw()
```

Step 4<br />
```{r}
#Step 4.
#Best looks like 7
pc_cluster_2 <-  kmeans(data_scaled, 7)

```

Step 5<br />
```{r}
##Step 5
data_scale = scale(data)
pc_cluster_3 <-  kmeans(data_scale, 7)

print(pc_cluster_3$cluster)
```

Step 6<br />
```{r}
##Step 6
clusplot(data_scaled, pc_cluster_2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
```

Step 7<br />
```{r}
##Step 7
#get centers
center <-pc_cluster_2$centers
# create dataset with the cluster number
cluster <- c(1:7)
center_df <- data.frame(cluster, center)
# Reshape the data
center_reshape <- gather(center_df, features, values, pop_scal: area_scal)
#Colors for plotting 
# Create the palette
hm.palette <-colorRampPalette(rev(brewer.pal(10, 'RdYlGn')),space='Lab')
#Visualize the clusters
#see what the clusters look like.
ggplot(data = center_reshape, aes(x = features, y = cluster, fill = values)) +
  scale_y_continuous(breaks = seq(1, 7, by = 1)) +
  geom_tile() +
  coord_equal() +
  scale_fill_gradientn(colours = hm.palette(90)) +
  theme_classic()

```
It is noticeable that population, income and area have the greatest variation among all of the attributes. If we know this I would say that they also have the greatest influence on the centers when running the clustering algorithm.

This is what the clustering algorithm looks like for every step on the data
```{r}
#This is what the clustering algorithm looks like for every step on the data
set.seed(2345)
# par(mfrow=c(2,1))
kmeans.ani(data_scaled, 3)
# dev.off()
```

I choose <br />
Shows creativity and excels above and beyond requirements<br />
I put this because I spent a lot of time trying to get all of the steps completed and making graphs that would better help me understand how the algorithm worked. I feel that I went above because did a heat map of the clusters and did an animation of the cluster algorithm on the state data so you could see whats happening step by step.

